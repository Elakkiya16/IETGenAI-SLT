{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "85d2a374",
      "metadata": {
        "id": "85d2a374"
      },
      "source": [
        "# 5.4 Fine-Tuning T5 for Gloss Generation\n",
        "This notebook fine-tunes `t5-small` on 500 samples from ISL_CLSRT to learn English-to-gloss conversion. The model is trained using HuggingFace Transformers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f9692904",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9692904",
        "outputId": "310aa261-373e-4072-82a6-d7edc41f21fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# ✅ Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "05423831",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05423831",
        "outputId": "16aa0ae4-3012-433b-acec-92f58ab4a006"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# ✅ Step 2: Install dependencies\n",
        "!pip install -q transformers datasets evaluate accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5d99879f",
      "metadata": {
        "id": "5d99879f"
      },
      "outputs": [],
      "source": [
        "# ✅ Step 3: Import libraries\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    T5Tokenizer, T5ForConditionalGeneration,\n",
        "    TrainingArguments, Trainer,\n",
        "    DataCollatorForSeq2Seq\n",
        ")\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "958e7abf",
      "metadata": {
        "id": "958e7abf"
      },
      "outputs": [],
      "source": [
        "# ✅ Step 4: Load and format dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/IETGenAI-SLT/Chapter 4/isl_train_meta.csv')\n",
        "df = df[['Sentences', 'gloss_sequence']].dropna().drop_duplicates().reset_index(drop=True)\n",
        "df['input_text'] = 'translate English to gloss: ' + df['Sentences']\n",
        "df = df.rename(columns={'gloss_sequence': 'target_text'})\n",
        "dataset = Dataset.from_pandas(df[['input_text', 'target_text']])\n",
        "dataset = dataset.train_test_split(test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43e7f2e4",
      "metadata": {
        "id": "43e7f2e4"
      },
      "outputs": [],
      "source": [
        "# ✅ Step 5: Tokenization\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "\n",
        "def tokenize_function(example):\n",
        "    model_inputs = tokenizer(example[\"input_text\"], max_length=64, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(example[\"target_text\"], max_length=32, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a2c5a245",
      "metadata": {
        "id": "a2c5a245"
      },
      "outputs": [],
      "source": [
        "# ✅ Step 6: Define model and training arguments\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"/content/t5-gloss-output\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=3e-4,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=1,\n",
        "    logging_dir=\"/content/t5-logs\",\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "73f99f8a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "id": "73f99f8a",
        "outputId": "27b0946a-5497-4e9e-a890-74ec223a01c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-9-811907638.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33melakkiya16\u001b[0m (\u001b[33melakkiya16-bits-pilani\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250723_084352-83te5lt0</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/elakkiya16-bits-pilani/huggingface/runs/83te5lt0' target=\"_blank\">/content/t5-gloss-output</a></strong> to <a href='https://wandb.ai/elakkiya16-bits-pilani/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/elakkiya16-bits-pilani/huggingface' target=\"_blank\">https://wandb.ai/elakkiya16-bits-pilani/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/elakkiya16-bits-pilani/huggingface/runs/83te5lt0' target=\"_blank\">https://wandb.ai/elakkiya16-bits-pilani/huggingface/runs/83te5lt0</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='55' max='55' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [55/55 03:33, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.221730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.932836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.704410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.592866</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.571278</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=55, training_loss=1.8061047640713779, metrics={'train_runtime': 226.7559, 'train_samples_per_second': 1.896, 'train_steps_per_second': 0.243, 'total_flos': 7274621829120.0, 'train_loss': 1.8061047640713779, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# ✅ Step 7: Trainer setup and training\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "27ab08f3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27ab08f3",
        "outputId": "593f1630-81e8-4f60-ff2f-d7be8b0b6f45"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/IETGenAI-SLT/Chapter 5/t5-gloss-finetuned/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/IETGenAI-SLT/Chapter 5/t5-gloss-finetuned/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/IETGenAI-SLT/Chapter 5/t5-gloss-finetuned/spiece.model',\n",
              " '/content/drive/MyDrive/IETGenAI-SLT/Chapter 5/t5-gloss-finetuned/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# ✅ Step 8: Save fine-tuned model\n",
        "model.save_pretrained('/content/drive/MyDrive/IETGenAI-SLT/Chapter 5/t5-gloss-finetuned')\n",
        "tokenizer.save_pretrained('/content/drive/MyDrive/IETGenAI-SLT/Chapter 5/t5-gloss-finetuned')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "cffd8c35",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cffd8c35",
        "outputId": "3e1df69b-8acf-4862-a9b1-0af365df1edf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Können Sie bitte langsamer sprechen?\n"
          ]
        }
      ],
      "source": [
        "# ✅ Step 9: Inference from fine-tuned model\n",
        "def generate_gloss(sentence):\n",
        "    prompt = f\"translate English to gloss: {sentence}\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    outputs = model.generate(**inputs, max_length=32)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(generate_gloss(\"Could you please talk slower?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "321a241a"
      },
      "source": [
        "# Task\n",
        "The model is translating the input sentences instead of generating glosses. This is likely due to the limited dataset size (500 sentences), which is insufficient for the model to learn the complex mapping from English sentences to gloss sequences. The model is defaulting to a more common task it might have been pre-trained on, like translation.\n",
        "\n",
        "To address this, I will:\n",
        "\n",
        "1.  **Inspect the dataset**: Ensure the gloss sequences are correctly formatted and aligned with the English sentences.\n",
        "2.  **Add evaluation metric**: Incorporate a suitable evaluation metric to monitor the model's performance during training.\n",
        "3.  **Experiment with training hyperparameters**: Adjust hyperparameters to improve training.\n",
        "4.  **Analyze model outputs during training**: Observe how the output changes over epochs.\n",
        "5.  **Consider data augmentation (optional)**: Explore techniques to augment the training data if necessary.\n",
        "6.  **Retrain the model**: Train the model with the adjusted configurations and data.\n",
        "7.  **Evaluate the fine-tuned model**: Evaluate the final model's performance and manually inspect generated glosses.\n",
        "8.  **Finish task**: Summarize the findings and the improved model's ability to generate glosses."
      ],
      "id": "321a241a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "148e6d89"
      },
      "source": [
        "## Inspect the dataset\n",
        "\n",
        "Carefully examine the input and target text in the training and test datasets to ensure the gloss sequences are correctly formatted and aligned with the English sentences.\n"
      ],
      "id": "148e6d89"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb77db0d",
        "outputId": "6c59a86c-71f7-47fb-ad02-1faded7dc8cf"
      },
      "source": [
        "print(\"First few examples from train split:\")\n",
        "for i in range(3):\n",
        "    print(f\"Example {i+1}:\")\n",
        "    print(f\"  input_text: {tokenized_datasets['train'][i]['input_text']}\")\n",
        "    print(f\"  target_text: {tokenized_datasets['train'][i]['target_text']}\")\n",
        "    print(f\"  decoded input_ids: {tokenizer.decode(tokenized_datasets['train'][i]['input_ids'], skip_special_tokens=True)}\")\n",
        "    print(f\"  decoded labels: {tokenizer.decode(tokenized_datasets['train'][i]['labels'], skip_special_tokens=True)}\")\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "print(\"\\nFirst few examples from test split:\")\n",
        "for i in range(3):\n",
        "    print(f\"Example {i+1}:\")\n",
        "    print(f\"  input_text: {tokenized_datasets['test'][i]['input_text']}\")\n",
        "    print(f\"  target_text: {tokenized_datasets['test'][i]['target_text']}\")\n",
        "    print(f\"  decoded input_ids: {tokenizer.decode(tokenized_datasets['test'][i]['input_ids'], skip_special_tokens=True)}\")\n",
        "    print(f\"  decoded labels: {tokenizer.decode(tokenized_datasets['test'][i]['labels'], skip_special_tokens=True)}\")\n",
        "    print(\"-\" * 20)"
      ],
      "id": "eb77db0d",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First few examples from train split:\n",
            "Example 1:\n",
            "  input_text: translate English to gloss: why are you crying\n",
            "  target_text:  YOU CRY WHY\n",
            "  decoded input_ids: translate English to gloss: why are you crying\n",
            "  decoded labels: YOU CRY WHY\n",
            "--------------------\n",
            "Example 2:\n",
            "  input_text: translate English to gloss: how dare you\n",
            "  target_text:  DARE YOU HOW\n",
            "  decoded input_ids: translate English to gloss: how dare you\n",
            "  decoded labels: DARE YOU HOW\n",
            "--------------------\n",
            "Example 3:\n",
            "  input_text: translate English to gloss: i am feeling cold\n",
            "  target_text: I FEEL COLD\n",
            "  decoded input_ids: translate English to gloss: i am feeling cold\n",
            "  decoded labels: I FEEL COLD\n",
            "--------------------\n",
            "\n",
            "First few examples from test split:\n",
            "Example 1:\n",
            "  input_text: translate English to gloss: let him take time\n",
            "  target_text: LET HIM TAKE TIME\n",
            "  decoded input_ids: translate English to gloss: let him take time\n",
            "  decoded labels: LET HIM TAKE TIME\n",
            "--------------------\n",
            "Example 2:\n",
            "  input_text: translate English to gloss: are you free today\n",
            "  target_text: YOU FREE TODAY\n",
            "  decoded input_ids: translate English to gloss: are you free today\n",
            "  decoded labels: YOU FREE TODAY\n",
            "--------------------\n",
            "Example 3:\n",
            "  input_text: translate English to gloss: how can i trust you\n",
            "  target_text: I TRUST YOU HOW \n",
            "  decoded input_ids: translate English to gloss: how can i trust you\n",
            "  decoded labels: I TRUST YOU HOW\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bea6e12"
      },
      "source": [
        "## Add evaluation metric\n",
        "\n",
        "Incorporate a suitable evaluation metric to monitor the model's performance during training and assess how well it's learning the English-to-gloss conversion.\n"
      ],
      "id": "7bea6e12"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8fc0969"
      },
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "metric = evaluate.load(\"bleu\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # Decode predictions and labels\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    # Replace -100 in labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # BLEU expects a list of references for each prediction\n",
        "    decoded_labels = [[label] for label in decoded_labels]\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    return result"
      ],
      "id": "c8fc0969",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbc0ed56",
        "outputId": "d341fed4-1b01-41f8-9e58-5a077cdcc26c"
      },
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "id": "dbc0ed56",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-14-493073965.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad6ef7b5"
      },
      "source": [
        "## Experiment with training hyperparameters\n",
        "\n",
        "Adjust hyperparameters like learning rate, batch size, or number of epochs to see if it improves training and prevents the model from defaulting to translation.\n"
      ],
      "id": "ad6ef7b5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DW9ZOefjf7_5",
        "outputId": "4cd8ec55-9426-4487-91db-939b7a6f9a89"
      },
      "source": [
        "# ✅ Step 3: Experiment with training hyperparameters\n",
        "\n",
        "# Update training arguments\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"/content/t5-gloss-output-tuned\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=1e-4,  # Lower learning rate\n",
        "    per_device_train_batch_size=4,  # Smaller batch size\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=10, # Increased epochs\n",
        "    weight_decay=0.01,\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=1,\n",
        "    logging_dir=\"/content/t5-logs-tuned\",\n",
        "    metric_for_best_model=\"bleu\", # Metric to use for best model\n",
        ")\n",
        "\n",
        "# Re-instantiate Trainer with updated arguments and the compute_metrics function\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "id": "DW9ZOefjf7_5",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-16-3330772917.py:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef431e23"
      },
      "source": [
        "## Retrain the model\n",
        "\n",
        "Train the model with the adjusted configurations and data.\n"
      ],
      "id": "ef431e23"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdM8DEQ6gNWQ",
        "outputId": "5a4fb20c-8bcf-472e-eef7-a94e909c1ac7"
      },
      "source": [
        "# ✅ Step 6: Define model and training arguments\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"/content/t5-gloss-output-tuned\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=1e-4,  # Lower learning rate\n",
        "    per_device_train_batch_size=4,  # Smaller batch size\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=10, # Increased epochs\n",
        "    weight_decay=0.01,\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=1,\n",
        "    logging_dir=\"/content/t5-logs-tuned\",\n",
        "    metric_for_best_model=\"bleu\", # Metric to use for best model\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "\n",
        "# Modify compute_metrics to handle generated sequences\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    # Decode predictions and labels\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    # Replace -100 in labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # BLEU expects a list of references for each prediction\n",
        "    decoded_labels = [[label] for label in decoded_labels]\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    return result\n",
        "\n",
        "# Re-instantiate Trainer with updated arguments and the modified compute_metrics function\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "id": "hdM8DEQ6gNWQ",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-18-2717156272.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "id": "EXJizUfYheKM",
        "outputId": "c9fe7525-29a8-4ca9-8fe5-2ad849bca4a0"
      },
      "source": [
        "# Modify compute_metrics to perform argmax on predictions (assuming they are logits)\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    # Assuming predictions is a tuple where the first element contains logits\n",
        "    if isinstance(predictions, tuple):\n",
        "        logits = predictions[0]\n",
        "    else:\n",
        "        # If not a tuple, assume predictions are logits directly\n",
        "        logits = predictions\n",
        "\n",
        "    # Perform argmax to get predicted token IDs\n",
        "    predicted_ids = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # Decode predictions and labels\n",
        "    decoded_preds = tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)\n",
        "    # Replace -100 in labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # BLEU expects a list of references for each prediction\n",
        "    decoded_labels = [[label] for label in decoded_labels]\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    return result\n",
        "\n",
        "# Re-instantiate Trainer with the modified compute_metrics function\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Initiate the training process\n",
        "trainer.train()"
      ],
      "id": "EXJizUfYheKM",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-24-85736165.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='220' max='220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [220/220 11:23, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Bleu</th>\n",
              "      <th>Precisions</th>\n",
              "      <th>Brevity Penalty</th>\n",
              "      <th>Length Ratio</th>\n",
              "      <th>Translation Length</th>\n",
              "      <th>Reference Length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.503120</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>[0.41379310344827586, 0.10526315789473684, 0.1, 0.0]</td>\n",
              "      <td>0.841631</td>\n",
              "      <td>0.852941</td>\n",
              "      <td>29</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.494047</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>[0.39285714285714285, 0.0, 0.0, 0.0]</td>\n",
              "      <td>0.807118</td>\n",
              "      <td>0.823529</td>\n",
              "      <td>28</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.462161</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>[0.4, 0.05, 0.0, 0.0]</td>\n",
              "      <td>0.875173</td>\n",
              "      <td>0.882353</td>\n",
              "      <td>30</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.442199</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>[0.41379310344827586, 0.05263157894736842, 0.0, 0.0]</td>\n",
              "      <td>0.841631</td>\n",
              "      <td>0.852941</td>\n",
              "      <td>29</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.445922</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>[0.3793103448275862, 0.05263157894736842, 0.0, 0.0]</td>\n",
              "      <td>0.841631</td>\n",
              "      <td>0.852941</td>\n",
              "      <td>29</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.432735</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>[0.3793103448275862, 0.05263157894736842, 0.0, 0.0]</td>\n",
              "      <td>0.841631</td>\n",
              "      <td>0.852941</td>\n",
              "      <td>29</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.420373</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>[0.4, 0.1, 0.0, 0.0]</td>\n",
              "      <td>0.875173</td>\n",
              "      <td>0.882353</td>\n",
              "      <td>30</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.427467</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>[0.3793103448275862, 0.05263157894736842, 0.0, 0.0]</td>\n",
              "      <td>0.841631</td>\n",
              "      <td>0.852941</td>\n",
              "      <td>29</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.425430</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>[0.3793103448275862, 0.05263157894736842, 0.0, 0.0]</td>\n",
              "      <td>0.841631</td>\n",
              "      <td>0.852941</td>\n",
              "      <td>29</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.424632</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>[0.3793103448275862, 0.05263157894736842, 0.0, 0.0]</td>\n",
              "      <td>0.841631</td>\n",
              "      <td>0.852941</td>\n",
              "      <td>29</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Trainer is attempting to log a value of \"[0.41379310344827586, 0.10526315789473684, 0.1, 0.0]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"[0.39285714285714285, 0.0, 0.0, 0.0]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"[0.4, 0.05, 0.0, 0.0]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"[0.41379310344827586, 0.05263157894736842, 0.0, 0.0]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"[0.3793103448275862, 0.05263157894736842, 0.0, 0.0]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"[0.3793103448275862, 0.05263157894736842, 0.0, 0.0]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"[0.4, 0.1, 0.0, 0.0]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"[0.3793103448275862, 0.05263157894736842, 0.0, 0.0]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"[0.3793103448275862, 0.05263157894736842, 0.0, 0.0]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"[0.3793103448275862, 0.05263157894736842, 0.0, 0.0]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=220, training_loss=0.40474742542613634, metrics={'train_runtime': 683.1289, 'train_samples_per_second': 1.259, 'train_steps_per_second': 0.322, 'total_flos': 14549243658240.0, 'train_loss': 0.40474742542613634, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83c4b520"
      },
      "source": [
        "## Evaluate the fine-tuned model\n",
        "\n",
        "Evaluate the final model's performance using the chosen metric and manually inspect generated glosses for correctness.\n"
      ],
      "id": "83c4b520"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "4b55b493",
        "outputId": "face02d1-03eb-49e7-e11e-f8f5b4d92833"
      },
      "source": [
        "# Evaluate the model on the test dataset\n",
        "eval_results = trainer.evaluate()\n",
        "print(\"Evaluation results:\", eval_results)\n",
        "\n",
        "# Define the generate_gloss function (already defined in a previous cell, but redefining for clarity in this step)\n",
        "def generate_gloss(sentence):\n",
        "    prompt = f\"translate English to gloss: {sentence}\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device) # Move to model's device\n",
        "    outputs = model.generate(**inputs, max_length=32)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Generate glosses for a few example sentences\n",
        "example_sentences = [\n",
        "    \"Could you please talk slower?\", # Sentence from previous inference\n",
        "    \"I am going to the market.\",\n",
        "    \"What is your name?\",\n",
        "    \"He is a good person.\"\n",
        "]\n",
        "\n",
        "print(\"\\nGenerated Glosses:\")\n",
        "for sentence in example_sentences:\n",
        "    gloss = generate_gloss(sentence)\n",
        "    print(f\"English: {sentence}\")\n",
        "    print(f\"Gloss: {gloss}\")\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "# Manual inspection of generated glosses will be done based on the output"
      ],
      "id": "4b55b493",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Trainer is attempting to log a value of \"[0.3793103448275862, 0.05263157894736842, 0.0, 0.0]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results: {'eval_loss': 0.42463159561157227, 'eval_bleu': 0.0, 'eval_precisions': [0.3793103448275862, 0.05263157894736842, 0.0, 0.0], 'eval_brevity_penalty': 0.8416308400672834, 'eval_length_ratio': 0.8529411764705882, 'eval_translation_length': 29, 'eval_reference_length': 34, 'eval_runtime': 1.7941, 'eval_samples_per_second': 5.574, 'eval_steps_per_second': 1.672, 'epoch': 10.0}\n",
            "\n",
            "Generated Glosses:\n",
            "English: Could you please talk slower?\n",
            "Gloss: YOU PLEASE TALK SLOWER?\n",
            "--------------------\n",
            "English: I am going to the market.\n",
            "Gloss: I GO TO THE MY MY MY MY.\n",
            "--------------------\n",
            "English: What is your name?\n",
            "Gloss: HOW DO YOU WANT WHAT\n",
            "--------------------\n",
            "English: He is a good person.\n",
            "Gloss: Er ist eine gute Person.\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a3ed62f"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The initial dataset inspection confirmed that the English sentences and gloss sequences were correctly formatted and aligned, eliminating data formatting issues as the primary cause of the model's translation behavior.\n",
        "*   Incorporating the BLEU metric revealed that the model achieved an `eval_bleu` score of 0.0 after training, indicating its inability to generate correct gloss sequences.\n",
        "*   Manual inspection of the generated glosses showed that the model is still producing incorrect translations or nonsensical sequences, although some outputs contained elements of glossing.\n",
        "*   Experimenting with hyperparameters (lower learning rate, smaller batch size, increased epochs) did not significantly improve the model's ability to perform gloss generation, as evidenced by the zero BLEU score.\n",
        "*   A technical challenge was encountered and resolved in the `compute_metrics` function to correctly handle the model's output (logits) during evaluation by applying `argmax` to get predicted token IDs.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The small dataset size (500 sentences) is likely the dominant factor preventing the model from learning the complex English-to-gloss mapping, overwhelming the impact of hyperparameter tuning. Significant data augmentation or acquiring a larger, high-quality dataset is crucial.\n",
        "*   Investigate alternative model architectures or pre-training strategies specifically designed for low-resource or specialized sequence generation tasks, as the current T5 model appears to default to its pre-trained translation capabilities.\n"
      ],
      "id": "9a3ed62f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "d858f12c",
        "outputId": "83c1ceec-91bd-4d20-a7aa-74602bb78fb3"
      },
      "source": [
        "# ✅ Step 9.1: Generate glosses for all sentences and save to DataFrame\n",
        "def generate_gloss(sentence):\n",
        "    prompt = f\"translate English to gloss: {sentence}\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(**inputs, max_length=32)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "df['generated_gloss'] = df['Sentences'].apply(generate_gloss)\n",
        "\n",
        "# Display the updated DataFrame with generated glosses\n",
        "display(df.head())"
      ],
      "id": "d858f12c",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                               Sentences                         target_text  \\\n",
              "0  it does not make any difference to me  IT MAKE ANY DIFFERENCE  ME DO NOT    \n",
              "1                          tell me truth                          TELL TRUTH   \n",
              "2                         do me a favour                       DO  FAVOUR ME   \n",
              "3                           do not worry                         DONOT WORRY   \n",
              "4                       do not abuse him                     HIM ABUSE DONOT   \n",
              "\n",
              "                                          input_text       generated_gloss  \n",
              "0  translate English to gloss: it does not make a...  I CAN NOT DIFFERENCE  \n",
              "1          translate English to gloss: tell me truth                I RÉAL  \n",
              "2         translate English to gloss: do me a favour         DO MONEY FOUR  \n",
              "3           translate English to gloss: do not worry        DO NOT HELP ME  \n",
              "4       translate English to gloss: do not abuse him          YOU MISS him  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9799dca2-d99a-45c1-8a5a-41db8340e19a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentences</th>\n",
              "      <th>target_text</th>\n",
              "      <th>input_text</th>\n",
              "      <th>generated_gloss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>it does not make any difference to me</td>\n",
              "      <td>IT MAKE ANY DIFFERENCE  ME DO NOT</td>\n",
              "      <td>translate English to gloss: it does not make a...</td>\n",
              "      <td>I CAN NOT DIFFERENCE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tell me truth</td>\n",
              "      <td>TELL TRUTH</td>\n",
              "      <td>translate English to gloss: tell me truth</td>\n",
              "      <td>I RÉAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>do me a favour</td>\n",
              "      <td>DO  FAVOUR ME</td>\n",
              "      <td>translate English to gloss: do me a favour</td>\n",
              "      <td>DO MONEY FOUR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>do not worry</td>\n",
              "      <td>DONOT WORRY</td>\n",
              "      <td>translate English to gloss: do not worry</td>\n",
              "      <td>DO NOT HELP ME</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>do not abuse him</td>\n",
              "      <td>HIM ABUSE DONOT</td>\n",
              "      <td>translate English to gloss: do not abuse him</td>\n",
              "      <td>YOU MISS him</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9799dca2-d99a-45c1-8a5a-41db8340e19a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9799dca2-d99a-45c1-8a5a-41db8340e19a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9799dca2-d99a-45c1-8a5a-41db8340e19a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-3c6743ea-490b-459a-9c03-c1a1854d3faa\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3c6743ea-490b-459a-9c03-c1a1854d3faa')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-3c6743ea-490b-459a-9c03-c1a1854d3faa button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Sentences\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"tell me truth\",\n          \"do not abuse him\",\n          \"do me a favour\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"TELL TRUTH\",\n          \" HIM ABUSE DONOT\",\n          \"DO  FAVOUR ME\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"input_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"translate English to gloss: tell me truth\",\n          \"translate English to gloss: do not abuse him\",\n          \"translate English to gloss: do me a favour\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"generated_gloss\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"I R\\u00c9AL\",\n          \"YOU MISS him\",\n          \"DO MONEY FOUR\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43dfa024",
        "outputId": "07caaa2b-31a6-4edb-e17a-3e634a9a8e03"
      },
      "source": [
        "# Define the path to save the CSV file\n",
        "output_path = '/content/drive/MyDrive/IETGenAI-SLT/Chapter 5/isl_train_meta_with_generated_glosses.csv'\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"DataFrame saved to {output_path}\")"
      ],
      "id": "43dfa024",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame saved to /content/drive/MyDrive/IETGenAI-SLT/Chapter 5/isl_train_meta_with_generated_glosses.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "233eeb8d"
      },
      "source": [
        "# ✅ Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ✅ Step 2: Install dependencies\n",
        "!pip install -q transformers datasets evaluate accelerate\n",
        "\n",
        "# ✅ Step 3: Import libraries\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    T5Tokenizer, T5ForConditionalGeneration,\n",
        "    TrainingArguments, Trainer,\n",
        "    DataCollatorForSeq2Seq\n",
        ")\n",
        "import torch\n",
        "\n",
        "# ✅ Step 4: Load and format dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/IETGenAI-SLT/Chapter 4/isl_train_meta.csv')\n",
        "df = df[['Sentences', 'gloss_sequence']].dropna().drop_duplicates().reset_index(drop=True)\n",
        "df['input_text'] = 'translate English to gloss: ' + df['Sentences']\n",
        "df = df.rename(columns={'gloss_sequence': 'target_text'})\n",
        "dataset = Dataset.from_pandas(df[['input_text', 'target_text']])\n",
        "dataset = dataset.train_test_split(test_size=0.1)\n",
        "\n",
        "# ✅ Step 5: Tokenization\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "\n",
        "def tokenize_function(example):\n",
        "    model_inputs = tokenizer(example[\"input_text\"], max_length=64, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(example[\"target_text\"], max_length=32, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Add evaluation metric and compute_metrics function\n",
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "metric = evaluate.load(\"bleu\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    # Assuming predictions is a tuple where the first element contains logits\n",
        "    if isinstance(predictions, tuple):\n",
        "        logits = predictions[0]\n",
        "    else:\n",
        "        # If not a tuple, assume predictions are logits directly\n",
        "        logits = predictions\n",
        "\n",
        "    # Perform argmax to get predicted token IDs\n",
        "    predicted_ids = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # Decode predictions and labels\n",
        "    decoded_preds = tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)\n",
        "    # Replace -100 in labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # BLEU expects a list of references for each prediction\n",
        "    decoded_labels = [[label] for label in decoded_labels]\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    return result\n",
        "\n",
        "# Define model and training arguments (tuned)\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"/content/t5-gloss-output-tuned\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=1e-4,  # Lower learning rate\n",
        "    per_device_train_batch_size=4,  # Smaller batch size\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=10, # Increased epochs\n",
        "    weight_decay=0.01,\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=1,\n",
        "    logging_dir=\"/content/t5-logs-tuned\",\n",
        "    metric_for_best_model=\"bleu\", # Metric to use for best model\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "\n",
        "# Trainer setup and training (with tuned arguments and compute_metrics)\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Save fine-tuned model\n",
        "model.save_pretrained('/content/drive/MyDrive/IETGenAI-SLT/Chapter 5/t5-gloss-finetuned')\n",
        "tokenizer.save_pretrained('/content/drive/MyDrive/IETGenAI-SLT/Chapter 5/t5-gloss-finetuned')\n",
        "\n",
        "# Inference from fine-tuned model and save generated glosses to DataFrame\n",
        "def generate_gloss(sentence):\n",
        "    prompt = f\"translate English to gloss: {sentence}\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(**inputs, max_length=32)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "df['generated_gloss'] = df['Sentences'].apply(generate_gloss)\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "eval_results = trainer.evaluate()\n",
        "print(\"Evaluation results:\", eval_results)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "output_path = '/content/drive/MyDrive/IETGenAI-SLT/Chapter 5/isl_train_meta_with_generated_glosses.csv'\n",
        "df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"DataFrame saved to {output_path}\")"
      ],
      "id": "233eeb8d",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}