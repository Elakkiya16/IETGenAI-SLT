{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d3bf0033",
      "metadata": {
        "id": "d3bf0033"
      },
      "source": [
        "# 5.2 Text to Gloss Generation using Flan-T5 (Few-shot Prompt)\n",
        "\n",
        "This notebook uses a few-shot prompt with real Sentence→Gloss pairs from ISL_CLSRT to generate sign language gloss from English sentences using the Flan-T5 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47ae465e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47ae465e",
        "outputId": "1e773793-f743-4b36-9240-ed5b8a769493"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# ✅ Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2951a157",
      "metadata": {
        "id": "2951a157"
      },
      "outputs": [],
      "source": [
        "# ✅ Step 2: Install Transformers\n",
        "#!pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52545f67",
      "metadata": {
        "id": "52545f67"
      },
      "outputs": [],
      "source": [
        "# ✅ Step 3: Import Libraries\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8ea127f",
      "metadata": {
        "id": "d8ea127f"
      },
      "outputs": [],
      "source": [
        "# ✅ Step 4: Load Flan-T5 Model\n",
        "model_name = \"google/flan-t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4a122eb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4a122eb",
        "outputId": "5be1f8c8-979b-4a08-93d8-bb08c430a937"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Gloss: Could you please talk slower?\n"
          ]
        }
      ],
      "source": [
        "# ✅ Step 5: Generate gloss using few-shot prompting\n",
        "prompt = \"\"\"Convert the sentence to sign language gloss:\n",
        "Example 1:\n",
        "Sentence: you are good\n",
        "Gloss: YOU GOOD\n",
        "Example 2:\n",
        "Sentence: it was nice chatting with you\n",
        "Gloss: YOU CHAT NICE\n",
        "Example 3:\n",
        "Sentence: Could you please talk slower?\n",
        "Gloss:\"\"\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "output = model.generate(**inputs, max_length=30)\n",
        "gloss = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(\"Generated Gloss:\", gloss)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81e12ca2"
      },
      "source": [
        "# ✅ Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ✅ Step 2: Install Transformers\n",
        "#!pip install -q transformers\n",
        "\n",
        "# ✅ Step 3: Import Libraries\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "# ✅ Step 4: Load Flan-T5 Model\n",
        "model_name = \"google/flan-t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# ✅ Step 5: Generate gloss using few-shot prompting\n",
        "prompt = \"\"\"Convert the sentence to sign language gloss:\n",
        "Example 1:\n",
        "Sentence: you are good\n",
        "Gloss: YOU GOOD\n",
        "Example 2:\n",
        "Sentence: it was nice chatting with you\n",
        "Gloss: YOU CHAT NICE\n",
        "Example 3:\n",
        "Sentence: Could you please talk slower?\n",
        "Gloss:\"\"\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "output = model.generate(**inputs, max_length=30)\n",
        "gloss = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(\"Generated Gloss:\", gloss)"
      ],
      "id": "81e12ca2",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}